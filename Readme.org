* Table of Contents                                                   :TOC_5:
- [[#project-specifications][Project Specifications]]
  - [[#inherited-project][Inherited Project]]
    - [[#components][Components]]
    - [[#the-overview-component][The Overview Component]]
      - [[#build-process][Build Process]]
  - [[#related-projects][Related Projects]]
- [[#database-choices][Database Choices]]
    - [[#cassandra][Cassandra]]
- [[#development-journal][Development Journal]]
  - [[#local-database-testing][Local Database Testing]]
    - [[#refactor-to-build-on-a-local-machine][Refactor to build on a local machine.]]
    - [[#implement-crud-services-for-the-endpoint-33][Implement CRUD services for the endpoint]]
    - [[#setup-cassandra-locally][Setup Cassandra locally]]
    - [[#connect-to-the-cassandra-from-the-server-77][Connect to the Cassandra from the server]]
    - [[#setup-postgres-locally][Setup Postgres locally]]
    - [[#connect-to-the-postgres-from-the-server-77][Connect to the Postgres from the server]]
    - [[#implement-modify-seeding-script-to-store-10-million-records-33][IMPLEMENT Modify seeding script to store 10 million records]]
    - [[#test-ensure-api-responds-within-50ms][TEST Ensure API responds within 50ms]]
    - [[#research-dbms-benchmarking][RESEARCH DBMS benchmarking]]
    - [[#implement-stress-test-using-new-relic-to-monitor][IMPLEMENT Stress test using New Relic to monitor]]
    - [[#research-choose-dbms-to-move-forward-with][RESEARCH Choose DBMS to move forward with]]
  - [[#deployment][Deployment]]
    - [[#deploy-service][Deploy service]]
    - [[#deploy-proxy][Deploy proxy]]
    - [[#seed-deployed-database][Seed deployed database]]
    - [[#stress-test-service][Stress test service]]
    - [[#stress-test-proxy][Stress test proxy]]
  - [[#scale-the-service][Scale the service]]

* Project Specifications

The goal of this project is to explore system design by inheriting a front end focused project from another team and scaling it. The inherited project used Mongo to serve 100 records and needed to be scaled to handle 10 million records with at least 1000 requests per second (RPS) with the goal of hitting 10,000 RPS. The inherited project had four separate services and a proxy service.

** Inherited Project

The project I inherited was a clone of a game landing page on [[https://store.steampowered.com/][Steam]].

#+html: <p align="center"><img src="steam_screenshot.png" /></p>

*** Components

There are four components:
  1. An overview of the game on the top right (my service)
  2. A photo carousel on the top middle of the page
  3. Information about the game below the carousel
  4. Reviews for the game below the game information

*** The Overview Component

#+html: <p align="center"><img src="overview_screenshot.png" /></p>

It utilizes the reviews component to display an overall user feeling for the game ranging from Overwhelmingly Positive to Overwhelmingly Negative as well as general information such as the devloper and user tags.

Tech stack for this component:
 - Server: node
 - Package management: npm
 - Client: react, webpack, babel
 - DB: Cassandra / Postgres
 - Testing: jest
 - Deployment: Docker, docker-compose

The component had a seeding script for adding 100 records to a Mongo database which I modified to seed millions to either Cassandra or Postgres databases.

It defines a single endpoint ~/api/overview/:gameId~ which origininally only supported GET requests. In order to fully explore the databases I updated the endpoint to support full CRUD operations using GET, POST, PUT, DELETE http methods.

**** Build Process

Building and deployment is done using docker-compose.

The docker-compose.yml file defines the build process for the component and database containers and defines the following environment variables:

| ENV VAR      | DESCRIPTION                         | VALUES                  |
|--------------+-------------------------------------+-------------------------|
| NODE_ENV     | Specifies which database URI to use | production, development |
| PROD_DB_HOST | Production database URI             | defaults to localhost   |
| PROD_API_URL | Production server URI               | defaults to localhost   |

Run ~docker-compose up~ and navigate to localhost in your browser. The service maps port 80 to the component container's port 3000. 

Building locally can be done by cloning the repository and running the following commands:
 1. ~npm install~
 2. ~npm run db:setup~ to seed the database. Requires ~mongod~ running.
 3. ~npm run start~ and navigate to localhost:300
 4. If making changes run ~npm run build~ to rebuild the bundle.

** Related Projects

Proxy Service: https://github.com/rpt15-drKarp/stephen_proxy

Teammates Servies:
 - https://github.com/rpt15-drKarp/Richard_Reviews
 - https://github.com/rpt15-drKarp/stephen_photoCarousel
 - https://github.com/rpt15-drKarp/Therese_aboutGame

* Database Choices

Before trying to scale the backend it was important to choose a database. My service used Mongo, which I have used quite a bit, so I decided to explore two others before making a final choice.

After checking job listings in my area, MySQL and PostgreSQL came up the most, so I wanted to use at least one. PostgreSQL supports arrays and MySQL does not --at least not directly-- which simplifies the schema I would need to design for the overview component.

I have heard lots of Cassandra hate/grief from my peers but also know senior software engineers who love it. I chose it simply because I want to see for myself.

Final Choices:
 1. PostgreSQL
 2. Cassandra

*** Cassandra

A distributed system with data replication for increased consistency. Looks like multiple copies of the data can be distributed lcoally or on multiple machines. A read then checks all copies and can compensate for bad data by comparing the responses. 

Terminology and concepts
 - Keyspace: The outermost container for data. Defines the properties that apply to the behavior of alll tables contained in the keyspace.
 - Cluster: A collection of nodes (possibly machines). I will only use one to start.
 - Replication factor: Defined in the keyspace, determines how many nodes act as copies of each row. Higher values mean more consistency but less performance.
 - Replication strategy: Defines how replicas will be placed. SimpleStrategy is used most often when nodes are on a single data center.
 - UDT: User defined types that usually have to be frozen when used in a table. Frozen types are serialzied into a single value and cannot be partially updated. Non-frozen types can have their individual fields updated independtly. I suspect a frozen UDT will improve performance on the host but decrease performance on the client.

One needs to use ~cqlsh~ to create keyspaces and tables. Instead of manually running the commands to setup a keyspace on a new system, you can run ~cqlsh -e "DESCRIBE KEYSPACE" > schema.cql~ to dump the ocommands to create the keysapce named by "KEYSPACE" to a file named schema.cql. To later setup that keyspace on a new environment run ~source schema.cql~.

The [[http://cassandra.apache.org/doc/latest/operating/hardware.html][official hardware recomendations]] suggest using no less than 2 gigs for the Java heap which should be no more than 50% of the system ram and having at least 2 CPU cores. I am unfortunately restricted to a t2.small EC2 instance for deployment which only has 2 gigs of ram and 1 CPU core.

* Development Journal

This sections describes my process and results for this project.

** Local Database Testing

*** DONE Refactor to build on a local machine.

Changed PROD_DB_HOST to localhost and refactored hard coded referenses to AWS services out of the client.

Updated tests to pass for the schema in use. The response from the /api/overview/:gameID endpoint is an array with a single object which I did not expect. I did not change this to prevent breaking compatibility with the other components who consume this API.

Client test failed to run because of a parsing error with Babel.

*** DONE Implement CRUD services for the endpoint [3/3]
   - [X] Post -> Set location header to the GET endpoint for the new record
   - [X] Delete
   - [X] Put

Use with endpoint:
 - Post -> /api/overview
 - Delete -> /api/overview/:gameId
 - Put -> /api/overview/:gameId

Used promise based api from Mongoose for brevity for the additional database methods despite the inhertied code using callbacks. The inherited code uses a callback to send messages to the client from the database module but doesn't set the appropriate headers and doesn't set status codes for errors. Since getting the callbacks to work isn't required for my work I will ignore them.

*** DONE Setup Cassandra locally

Install process for Fedora via ~dnf~
 1. Run ~dnf install cassandra cassandra-server~
 2. Run ~systemctl start cassandra~
 3. Run ~systemctl enable cassandra~

Manually connect by running ~cqlsh~. 

I initially did not run ~systemctl start cassandra~ and could not connect via cqlsh. ~systemctl enable~ will automatically start the process on a reboot but not for the current session. The shell gave a very helpful message about not finding any servers to connect to. +1 for the helpful error.

I connected to Cassandra through ~cqlsh~ and setup a new keyspace by running:

 - ~CREATE KEYSPACE overviews WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}~ SimpleStrategy because I will only be using one node and replication factor 3 because I think anything less defeats the point of Cassandra. I may have to come back to this.

I created a new UDT for an overview so that I can easily pull this out as a javascript object later.

#+BEGIN_SRC cql
CREATE TYPE overviews.overview (
    game_id int,
    game_name text,
    description text,
    release_date text,
    developer text,
    publisher text,
    tags list<text>
);
#+END_SRC

I then defined a new table to store my rows.

#+BEGIN_SRC cql
CREATE TABLE overviews.overview (
    game_id int PRIMARY KEY,
    game frozen<overview>
);
#+END_SRC

I exported this schema to a file for a quicker setup later by running ~cqlsh -e "DESCRIBE overviews" > overviews.cql~.

*** DONE Connect to the Cassandra from the server [7/7]

Cassandra can be connected to from Node using the [[https://www.npmjs.com/package/cassandra-driver][cassandra-driver]] package.

The client connection requires three paramaters.
 1. The contact points which I found by running ~nodetool status~ as root and looking for the host address.
 2. The local data center which [[https://stackoverflow.com/questions/19489498/getting-cassandra-datacenter-name-in-cqlsh][StackOverflow]] informed me I can find by going into the cqlsh and running ~use system;~ followed by ~select data_center from local;~.
 3. The keyspace name

Queries are stored as string and passed as parameters to the execute function of the client object. Mutliple parameters must be passed as an array and named paramters require preparing the query by passing ~{ prepare: true }~ after the paramters. Preparing the query allows for converting Javscript objects to a Cassandra type. 

Numbers also have to be prepared as Javascript's 64bit float values don't directly map to Java's (and thus Cassandra's) Long values. cassandra-driver uses the [[https://www.npmjs.com/package/long][long]] package to handle values to and from Cassandra.

Functions to implement:
 - [X] save
 - [X] count     // used by the seeding script to determine if it should run
 - [X] retrieve
 - [X] update
 - [X] remove

Because an INSERT is also an upsert, my save and update functions are identical (or rather update just calls save). Because it doesn't break the clients and is more performant then checking if a record exists before an update, I decided to leave it as is.

Retrieve needed to wrap it's result in an array to maintain compatibility with the clients.

 - [X] Confirm client code is working
 - [X] Confirm tests pass

*** DONE Setup Postgres locally

Install process for Fedora via ~dnf~:
 1. Run ~dnf install postgresql-server postgresql-contrib~
 2. Run ~postgresql-setup --initdb --unit postgresql~
 3. Run ~systemctl enable postgresql~
 4. Run ~systemctl start postgresql~

Manually connect to Postgres by running ~psql~.

I initially did not have step 2 and I tried to start the postgresql service but it would not run. Checking journalctl showed that it failed to start the database server. Some quick Googling revealed that I needed to setup the database by creating a data directory, setting the ownership to the postgres user, and initializing the database as the postgres user. Turns out there is also a Fedora package called ~postgresql-setup~ which can be used to do the same thing which is installed along with Postgres. I opted for the package.

Running ~psql~ requires the user to have role setup so I was not able to run the shell from my user. The documentation says to first ~su - postgres~ but that cannot be done as the install process creates the local postgres user without a password. Instead I ran ~sudo -u postgres -i~ to effectively log in as that user in my shell and run ~psql~.

I was unable to conenct to the Postgres database as it was using ident authentication and node-postgres seems to only use password authentication. I changed ~pg_hba.conf~ settings for host from ident auth to md5. I was still unable to connect as the default postgres role does not have a password. I ran ~psql~ and set as new password with ~ALTER USER postgress PASSWORD 'the password';~ and was finally able to connect from node-postgres.

I created a database with:
#+BEGIN_SRC SQL
CREATE DATABASE overviews;
#+END_SRC
and a table with:
#+BEGIN_SRC SQL
CREATE TABLE overviews (
  game_id INTEGER PRIMARY KEY,
  game_name TEXT NOT NULL,
  description TEXT NOT NULL,
  release_date TEXT NOT NULL,
  developer TEXT NOT NULL,
  publisher TEXT NOT NULL,
  tags TEXT ARRAY
);
#+END_SRC

*** DONE Connect to the Postgres from the server [7/7]

I chose to use the [[https://www.npmjs.com/package/pg][node-postres]] package to connect to my Postgres database as Sequelize and even Knex are overkill for this project.

Functions to implement:
 - [X] save
 - [X] count     // used by the seeding script to determine if it should run
 - [X] retrieve
 - [X] update
 - [X] remove

In Cassandra, the insert query handles updates automatically, allowing the user to pass the game_id and then whatever columns need to be updated. Doing in the same thing with a SQL query required building up the query by iterating through the passed in objects' keys in order to determine exactly what fields need to be updated.

 - [X] Confirm client code is working
 - [X] Confirm tests pass

*** IMPLEMENT Modify seeding script to store 10 million records [3/3]
    - [X] Modular function for generating 10 million records
    - [X] Cassandra save script

Started with a humble amount of 1 million records and node ran out of memory. In order to constrain the number of promises and not blow the stack I installed [[https://www.npmjs.com/package/bluebird][bluebird]] and made use of its [[http://bluebirdjs.com/docs/api/promise.map.html][Promise.map]] function which allowed me specify the number of concurrent Promises. I also had to refactor the for loop which generated the random rows into an iterator for Promise.map and factor the body of the for loop into a helper function.

After refactoring to Promise.map my seed script succesfully seeded 1 million records to a Cassandra database but it took about four and a half minutes with a concurrency value of 10. Increasing the concurrent promises to 100 only took off 30 seconds.

I tried another approach  using the [[https://docs.datastax.com/en/developer/nodejs-driver/4.2/api/module.concurrent/][executeConcurrent]] function from cassandra-driver to concurrently run inserts with a batch size of 10,000. This approach finished in two and a half minutes and used about 200MB less memory. Seeding a full 10 million records takes around 25 minutes.

The concurrent_writes parameter in the cassandra.yaml configuration file defines how many concurrent writes can be done at once. The documentation suggests 8 * the number of cpu cores which is 32 for my machine. Since I will be deploying to a t2.small with one core I can expect it to take longer to seed once deployed. 

    - [X] Postgres save script

The [[https://www.postgresql.org/docs/11/populate.html][official documentation]] has some tips for inserting a lot of data. Since I'm generating my seed data in code, using ~COPY~ doesn't seem lke the best option. I'm willing to wait a little longer to seed if it simplifies the process. The best approach seems to be to turn off autocommit and indexes, insert in one transaction using ~BEGIN~ at the start, ~COMMIT~ at the end, and then build the indexes. When using ~BEGIN~ and ~COMMIT~ I cannot use a conenction pool as all queries need to use the client connection to be on a single transaction. Because the inserts will be done in a single transaction, if there are any errors nothing will be added to the database, which means I wont have to drop the table if the seeding fails. Spiffy.

Additionaly I want to prepare an ~INSERT~ and then use that prepared statement thereafter when seeding. [[https://github.com/brianc/node-postgres/issues/24][From the author]] of node-postgres, named queries are parsed, bound, and executed all at once, but subsequent queries issued on the same connection will skip the parsing so I don't need to do anything extra for that speedup.

Using a single transaction with indexing delayed and built at the end, inserting 1 million records took four and half minutes which means 10 million would take arounf 45 minutes. I wanted to get the seeding down to under half an hour and discovered that Postgres can [[https://www.postgresql.org/docs/11/sql-insert.html][insert multiple rows]] at once. It seems the number of rows you can insert at once is limited by the query size in characters.

I added a function to insert multiple rows at once using [[https://github.com/datalanche/node-pg-format][pg-format]] to format the query as node-postgress doesn't support it. pg-format didn't format array parameters correctly so I had to convert the tag array into postgres literal string before calling pg-format.

The final seeding function using a single transaction, defering indexing, and inserting 1000 rows per query seeded 1 million records in 2.3 minutes. The final 10 million record seeding took 24 minutes.

*** TEST Ensure API responds within 50ms
 - [ ] Cassandra
 - [ ] Postgres

*** RESEARCH DBMS benchmarking

*** IMPLEMENT Stress test using New Relic to monitor

| DBMS      | ROUTE |  RPS | LATENCY | ERROR RATE |
|-----------+-------+------+---------+------------|
| Cassandra | GET   |    1 |         |            |
| Cassandra | GET   |   10 |         |            |
| Cassandra | GET   |  100 |         |            |
| Cassandra | GET   | 1000 |         |            |
| Cassandra | POST  |    1 |         |            |
| Cassandra | POST  |   10 |         |            |
| Cassandra | POST  |  100 |         |            |
| Cassandra | POST  | 1000 |         |            |
| Postgres  | GET   |    1 |         |            |
| Postgres  | GET   |   10 |         |            |
| Postgres  | GET   |  100 |         |            |
| Postgres  | GET   | 1000 |         |            |
| Postgres  | POST  |    1 |         |            |
| Postgres  | POST  |   10 |         |            |
| Postgres  | POST  |  100 |         |            |
| Postgres  | POST  | 1000 |         |            |

*** RESEARCH Choose DBMS to move forward with

** Deployment

*** Deploy service

*** Deploy proxy

*** Seed deployed database

*** Stress test service

*** Stress test proxy

** Scale the service
